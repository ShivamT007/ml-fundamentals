{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14355483",
   "metadata": {},
   "source": [
    "# Model Evaluation and Selection\n",
    "\n",
    "In this notebook, we will learn how to evaluate and select regression models using the supermarket sales dataset.\n",
    "\n",
    "**Objectives:**\n",
    "- Understand key evaluation metrics\n",
    "- Compare different regression models\n",
    "- Use interactive tools to select the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea2918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Load dataset\n",
    "file_path = '../../data/SuperMarketAnalysis.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ee34c2",
   "metadata": {},
   "source": [
    "## Understanding Model Evaluation\n",
    "\n",
    "### What is Model Evaluation?\n",
    "\n",
    "Model evaluation is the process of assessing how well a machine learning model performs on unseen data. It helps us:\n",
    "\n",
    "1. **Compare different models** to select the best one for our problem\n",
    "2. **Understand model performance** and identify potential issues\n",
    "3. **Make informed decisions** about model deployment\n",
    "4. **Detect overfitting or underfitting** in our models\n",
    "\n",
    "### Evaluation Metrics for Regression\n",
    "\n",
    "- **Mean Squared Error (MSE):** Measures average squared difference between actual and predicted values.\n",
    "- **Mean Absolute Error (MAE):** Measures average absolute difference.\n",
    "- **R² Score:** Proportion of variance explained by the model.\n",
    "\n",
    "### Why Multiple Metrics for Regression?\n",
    "\n",
    "Unlike classification where accuracy might suffice, regression requires multiple metrics because:\n",
    "\n",
    "**1. Different Perspectives on Error:**\n",
    "- Some metrics focus on **large errors** (MSE) while others treat all errors equally (MAE)\n",
    "- Different business contexts may prioritize different types of errors\n",
    "\n",
    "**2. Scale Sensitivity:**\n",
    "- MSE is in squared units, making it harder to interpret\n",
    "- MAE is in original units, more intuitive\n",
    "- R² is unitless, good for comparing across different datasets\n",
    "\n",
    "**3. Outlier Sensitivity:**\n",
    "- **MSE** heavily penalizes large errors (outliers have huge impact)\n",
    "- **MAE** is more robust to outliers\n",
    "- **R²** can be misleading with outliers\n",
    "\n",
    "### How to Choose the Right Metric:\n",
    "\n",
    "| Metric | Use When | Advantages | Disadvantages |\n",
    "|--------|----------|------------|---------------|\n",
    "| **MSE** | Large errors are very costly | Penalizes big mistakes heavily | Sensitive to outliers, hard to interpret |\n",
    "| **MAE** | All errors are equally important | Easy to interpret, robust to outliers | Doesn't distinguish between small and large errors |\n",
    "| **R²** | Want to understand explained variance | Scale-independent, easy to understand (0-1) | Can be misleading, doesn't show error magnitude |\n",
    "\n",
    "### Best Practice:\n",
    "**Always use multiple metrics together** to get a complete picture of model performance. A model that looks good on one metric might perform poorly on another!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc45a90",
   "metadata": {},
   "source": [
    "Let's compute these metrics for different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21006474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "features = ['Unit price', 'Branch', 'Product line', 'Gender', 'Customer type']\n",
    "df_encoded = pd.get_dummies(df[features], drop_first=True)\n",
    "X = df_encoded\n",
    "y = df['Sales']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit models\n",
    "def fit_and_evaluate(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return mse, mae, r2\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1, max_iter=10000)\n",
    "}\n",
    "\n",
    "results = {name: fit_and_evaluate(model) for name, model in models.items()}\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results, index=['MSE', 'MAE', 'R2']).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cced005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive model selection\n",
    "@widgets.interact(model=list(models.keys()))\n",
    "def show_metrics(model):\n",
    "    mse, mae, r2 = results[model]\n",
    "    print(f\"{model} Results:\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R²: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5429fe7",
   "metadata": {},
   "source": [
    "## Visual Comparison\n",
    "\n",
    "Let's visualize the predictions of each model against the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac4ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    plt.scatter(y_test, y_pred, label=name, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual Total')\n",
    "plt.ylabel('Predicted Total')\n",
    "plt.title('Actual vs Predicted Sales')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5942a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    plt.scatter(y_test, y_pred, label=name, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual Total')\n",
    "plt.ylabel('Predicted Total')\n",
    "plt.title('Actual vs Predicted Sales')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e29ca50",
   "metadata": {},
   "source": [
    "As shown in the graph above, the three regression models perform similarly. It may be worthwhile to explore more advanced models that can capture more complex patterns and potentially improve our evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f2fae2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored the fundamental concepts of model evaluation for regression problems:\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "**1. Model Evaluation Purpose:**\n",
    "- Helps compare different models objectively\n",
    "- Identifies potential overfitting or underfitting issues\n",
    "- Guides informed decisions about model deployment\n",
    "\n",
    "**2. Multiple Metrics Are Essential:**\n",
    "- **MSE**: Best when large errors are costly (penalizes outliers heavily)\n",
    "- **MAE**: Ideal when all errors are equally important (robust to outliers)\n",
    "- **R²**: Useful for understanding explained variance (scale-independent)\n",
    "\n",
    "**3. Practical Implementation:**\n",
    "- We compared Linear, Ridge, and Lasso regression on supermarket sales data\n",
    "- All three models performed similarly, showing the dataset's linear nature\n",
    "- Interactive widgets enable dynamic model comparison\n",
    "\n",
    "**4. Visual Analysis:**\n",
    "- Scatter plots of actual vs predicted values reveal model performance patterns\n",
    "- The diagonal line represents perfect predictions\n",
    "- Similar clustering of all models indicates comparable performance\n",
    "\n",
    "### Best Practices:\n",
    "- Always use multiple evaluation metrics together\n",
    "- Consider your business context when choosing which metrics to prioritize\n",
    "- Visualize predictions to gain intuitive understanding of model behavior\n",
    "- Use interactive tools to explore different model characteristics\n",
    "\n",
    "### Next Steps:\n",
    "Learn about cross-validation techniques for more robust and reliable model evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
