{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4eaf58d",
   "metadata": {},
   "source": [
    "# Cross-Validation for Reliable Model Assessment\n",
    "\n",
    "This notebook introduces [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)), a powerful technique to assess model performance more reliably and evaluate how well a model will generalize to unseen data.\n",
    "\n",
    "**When to use it?**\n",
    "\n",
    "Cross-validation helps you:\n",
    "- Estimate generalization error more reliably than a single train/test split.\n",
    "- Diagnose model issues like underfitting or overfitting.\n",
    "- Select the best model or hyperparameters, including regularization strength.\n",
    "\n",
    "**Objectives:**\n",
    "- Understand cross-validation concepts\n",
    "- Apply k-fold cross-validation to regression models\n",
    "- Use interactive widgets to explore results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9296637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"../../data/SuperMarketAnalysis.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0534c7",
   "metadata": {},
   "source": [
    "## What is Cross-Validation?\n",
    "\n",
    "Cross-validation splits the data into multiple parts (folds), trains the model on some folds, and tests it on the remaining. This process is repeated to get a more robust estimate of model performance.\n",
    "\n",
    "**K-Fold Cross-Validation:** \n",
    "1. Divide your data into k equal parts (folds)\n",
    "2. For each fold: \n",
    "    - train the model on k-1 folds\n",
    "    - test on the remaining fold\n",
    "3. Average the performance across all k folds\n",
    "\n",
    "![cross_validation](../images/cross_validation.png)\n",
    "\n",
    "Let's apply k-fold cross-validation to our regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab355be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "features = [\"Unit price\", \"Branch\", \"Product line\", \"Gender\", \"Customer type\"]\n",
    "df_encoded = pd.get_dummies(df[features], drop_first=True)\n",
    "X = df_encoded\n",
    "y = df[\"Sales\"]\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(alpha=1.0),\n",
    "    \"Lasso Regression\": Lasso(alpha=0.1, max_iter=10000),\n",
    "}\n",
    "\n",
    "# Set up K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_results = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "    cv_results[name] = -scores\n",
    "\n",
    "# Visualize\n",
    "results_df = pd.DataFrame(cv_results)\n",
    "results_df.plot(kind=\"box\", figsize=(8, 5))\n",
    "plt.title(\"Cross-Validated MSE for Each Model\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42d089",
   "metadata": {},
   "source": [
    "During cross-validation, we use `neg_mean_squared_error` because scikit-learn's `cross_val_score` convention is that higher scores are better. \n",
    "Since mean squared error (MSE) is a loss (lower is better), it is returned as a negative value. We negate it later to get positive MSE values for interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a03b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive: Select model and see fold scores\n",
    "@widgets.interact(model=list(models.keys()))\n",
    "def show_cv_scores(model):\n",
    "    scores = cv_results[model]\n",
    "    print(f\"{model} CV MSE scores: {scores.round(2)}\")\n",
    "    print(f\"Mean MSE: {scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23a705e",
   "metadata": {},
   "source": [
    "Although MSE values remain high, we notice that the scores per model are not too far off from each other. Therefore it seems like our model is not unstable and perform similarly on seen and unseen data. If we look back to [01_regression.ipynb](01_regression.ipynb), MSE was high for both train and test set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986eecce",
   "metadata": {},
   "source": [
    "## Why use cross-validation?\n",
    "- Better model evaluation: reduces the chance that your train/test split gives misleading performance\n",
    "- Use more data for training: every sample gets to be in the test set once and training set k-1 times\n",
    "- More robust and helps detect overfitting: if test performance varies a lot across folds, your model might be overfitting or is might be unstable\n",
    "\n",
    "## When to use cross-validation?\n",
    "- You want reliable evluation on limited data\n",
    "- You're comparing models or hyperparameters\n",
    "- You're tuning hyperparameters\n",
    "- You don't have a separate validation set\n",
    "\n",
    "## When NOT to use cross-validation?\n",
    "- On huge datasets where training k times is too expensive\n",
    "\n",
    "## When NOT to use simple K-fold cross-validation?\n",
    "- When the data is time-dependent. In this case you should use [time-series cross-validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html).\n",
    "- When the data is split into groups. For example, if you're interested in predicting some biomarkers for an unseen patient, and in your data you have \n",
    "samples (observations) from the same patient (group), you want to make sure that the test and train sets don't share the same patient. In this case, you need to use [Group cross-validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da856a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- Cross-validation provides a more reliable estimate of model performance\n",
    "- Use it to compare models and avoid overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
