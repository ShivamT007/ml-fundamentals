{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f704ea",
   "metadata": {},
   "source": [
    "# Advanced Regression Algorithms\n",
    "\n",
    "This notebook explores more advanced regression algorithms beyond Linear Regression, Ridge, and Lasso. We will use the supermarket sales dataset and compare the performance of several sophisticated models.\n",
    "\n",
    "**Objectives:**\n",
    "- Introduce advanced regression algorithms\n",
    "- Compare their performance using standard metrics\n",
    "- Discuss practical considerations for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e215433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70970f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = \"../../data/SuperMarketAnalysis.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a424614",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We will use the same features as before and apply one-hot encoding to categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6f050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "# Select features (excluding target and identifiers)\n",
    "features_to_encode = [\n",
    "    \"Branch\",\n",
    "    \"City\",\n",
    "    \"Customer type\",\n",
    "    \"Gender\",\n",
    "    \"Product line\",\n",
    "    \"Payment\",\n",
    "]\n",
    "\n",
    "other_features_to_include = [\n",
    "    \"Unit price\",\n",
    "    \"Rating\",\n",
    "]\n",
    "\n",
    "# Encode categorical variables\n",
    "df_encoded = pd.concat(\n",
    "    [\n",
    "        pd.get_dummies(df[features_to_encode], drop_first=True),\n",
    "        df[other_features_to_include],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "X = df_encoded\n",
    "y = df[\"Sales\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e597ad7d",
   "metadata": {},
   "source": [
    "## Why Use Advanced regression models?\n",
    "\n",
    "### Our Scenario: Supermarket sales prediction\n",
    "\n",
    "In our previous notebooks, we discovered that **linear models had high MSE on both training and test sets**, indicating **underfitting**. This suggests our supermarket sales data has complex patterns that linear models cannot capture effectively.\n",
    "\n",
    "### Why linear models fall short:\n",
    "\n",
    "1. **Non-linear Relationships**: sales might depend on complex interactions between features\n",
    "   - Example: High-priced items might sell differently across branches\n",
    "   - Customer type + product line combinations might have unique patterns\n",
    "\n",
    "2. **Feature Interactions**: real-world sales involve feature combinations\n",
    "   - Weekend + certain product lines = higher sales\n",
    "   - Gender + product preferences = complex buying patterns\n",
    "\n",
    "3. **Complex Business Logic**: supermarket sales involve:\n",
    "   - Seasonal patterns, customer behavior, location effects\n",
    "   - Price sensitivity varies by product category\n",
    "   - Branch-specific customer preferences\n",
    "\n",
    "Advanced Models can capture:\n",
    "- **Non-linear patterns** in the data\n",
    "- **Feature interactions** automatically\n",
    "- **Complex decision boundaries**\n",
    "- **Better predictions** for business decision-making\n",
    "\n",
    "### Why these specific models?\n",
    "\n",
    "| Model | Why Use for Supermarket Sales | Key Strengths |\n",
    "|-------|------------------------------|---------------|\n",
    "| **Random Forest** | Robust ensemble method, good baseline for business data | Handles mixed data types, reduces overfitting, provides feature importance |\n",
    "| **Gradient Boosting** | Sequential learning captures complex patterns | Excellent predictive performance, handles non-linearity well |\n",
    "| **XGBoost** | Industry standard for structured data like sales records | Optimized performance, built-in regularization, handles missing values |\n",
    "| **LightGBM** | Fast training on business datasets | Memory efficient, excellent for categorical features, fast inference |\n",
    "\n",
    "### Business impact\n",
    "Better sales predictions help with:\n",
    "- **Inventory management**: stock the right products\n",
    "- **Revenue forecasting**: plan budgets and targets\n",
    "- **Customer insights**: understand buying patterns\n",
    "- **Branch optimization**: tailor strategies per location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2861c97",
   "metadata": {},
   "source": [
    "## Advanced Regression Models\n",
    "\n",
    "Let's explore each model and understand why it's suitable for our supermarket sales prediction:\n",
    "\n",
    "### ðŸŒ³ **[Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)**\n",
    "- **How it works**: Combines multiple decision trees, each trained on different data subsets\n",
    "- **Why for sales data**: Excellent at capturing feature interactions (e.g., branch + product line effects)\n",
    "- **Advantages**: Robust to outliers, provides feature importance, reduces overfitting\n",
    "\n",
    "### ðŸ“ˆ **[Gradient Boosting Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)**\n",
    "- **How it works**: Builds models sequentially, each correcting errors from previous ones\n",
    "- **Why for sales data**: Learns complex non-linear patterns in customer behavior\n",
    "- **Advantages**: High predictive accuracy, handles mixed data types well\n",
    "\n",
    "### âš¡ **[XGBoost (Extreme Gradient Boosting)](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/built-in-models/xgboost)**\n",
    "- **How it works**: Optimized gradient boosting with advanced regularization\n",
    "- **Why for sales data**: Industry standard for structured business data like ours\n",
    "- **Advantages**: Built-in regularization, handles missing values, excellent performance\n",
    "\n",
    "### ðŸš€ **[LightGBM (Light Gradient Boosting Machine)](https://lightgbm.readthedocs.io/en/latest/index.html)**\n",
    "- **How it works**: Gradient boosting optimized for speed and memory efficiency\n",
    "- **Why for sales data**: Excels with categorical features (branch, product line, gender)\n",
    "- **Advantages**: Fast training, memory efficient, great for categorical data\n",
    "\n",
    "All these models can automatically discover complex patterns that linear regression missed in our supermarket sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3120d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, random_state=42, verbosity=0),\n",
    "    \"LightGBM\": LGBMRegressor(n_estimators=100, random_state=42, verbose=-1),\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "}\n",
    "\n",
    "\n",
    "def fit_and_evaluate(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return mse, mae, r2\n",
    "\n",
    "\n",
    "results = {name: fit_and_evaluate(model) for name, model in models.items()}\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results, index=[\"MSE\", \"MAE\", \"R2\"]).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f893af24",
   "metadata": {},
   "source": [
    "## Visual Comparison\n",
    "\n",
    "Let's visualize the predictions of each model against the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7611176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    plt.scatter(y_test, y_pred, label=name, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"k--\", lw=2)\n",
    "plt.xlabel(\"Actual Sales\")\n",
    "plt.ylabel(\"Predicted Sales\")\n",
    "plt.title(\"Actual vs Predicted Sales (Advanced Models)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a94bb8",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "**<span style=\"color: green;\">QUESTION:</span>** Which model performed best on this dataset?\n",
    "\n",
    "<details>\n",
    "        <summary>Answer: Click to show</summary>\n",
    "\n",
    "It's Linear Regression! This could be due to the specific characteristics of the dataset, such as linear relationships between features and the target variable, or the absence of complex interactions that more advanced models typically capture. Additionally, simpler models like Linear Regression can sometimes outperform more complex models when the dataset is small.\n",
    "\n",
    "MAE treats all errors equally by averaging the absolute values of the residuals. MSE squares the errors, so it penalizes large errors much more than small ones. R squared measures how well the model explains the variance in the data and is not as sensitive to outliers as the MSE\n",
    "</details>\n",
    "\n",
    "**<span style=\"color: green;\">QUESTION:</span>** What could be a next step in understanding further model performances?\n",
    "\n",
    "<details>\n",
    "        <summary>Answer: Click to show</summary>\n",
    "Error analysis are possible next steps after training the model to further optimize it and to understand why it makes mistakes and how to improve it.\n",
    "</details>\n",
    "\n",
    "**<span style=\"color: green;\">QUESTION:</span>** What are the trade-offs between model complexity, interpretability, and performance?\n",
    "\n",
    "<details>\n",
    "        <summary>Answer: Click to show</summary>\n",
    "Thereâ€™s a trade-off between model complexity, interpretability, and performance. Simple models are easier to understand and explain but may underperform on complex tasks, while complex models can capture more patterns but are harder to interpret and risk overfitting. The best choice depends on the problemâ€”some situations demand transparency, while others prioritize predictive accuracy\n",
    "\n",
    "In our case, the simple model performed best, so there is no tradeoff to be made!\n",
    "</details>\n",
    "\n",
    "As we notice from the above explorations, sometimes more sophisticated methods do not always outperform simpler ones like linear regression. In many cases, **straightforward models can achieve strong results with less complexity**.\n",
    "Before moving on to more advanced models that can capture complex patterns and potentially improve the evaluation metrics, it may be worthwhile to take a step back and conduct an error analysis, to understand where we can truly improve on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d6c109",
   "metadata": {},
   "source": [
    "# Additional Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62da4fc",
   "metadata": {},
   "source": [
    "## Regularization: Ridge and Lasso Regression\n",
    "\n",
    "Regularization techniques help prevent overfitting by adding a penalty to the regression model. This is especially useful when dealing with many features, our model is overfitting or multicollinearity. \n",
    "\n",
    "- **Ridge Regression:** Adds L2 penalty (squared magnitude of coefficients). L2 penalty shrinks the coefficients of the variables, but never makes them zero (disappear completely from the quation). It keeps all features but reduces their influence, spreading effects more evenly.\n",
    "- **Lasso Regression:** Adds L1 penalty (absolute value of coefficients), can shrink some coefficients to zero (feature selection). L1, on the other hand, pushes some coefficients to zero. It does automatically variable sleection and keeps only the most important independent variables in the final equation. In machine learning we say that L1 does _feature (or variable) selection_\n",
    "\n",
    "These methods help improve model generalization and are widely used in business analytics and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eee275",
   "metadata": {},
   "source": [
    "**<span style=\"color: green;\">QUESTION:</span>** What is overfitting?\n",
    "<details>\n",
    "        <summary>Answer: Click to show</summary>\n",
    "Opposite to underfitting, overfitting happens when a model learns not only the true pattern in the trainnig data, but also random fluctuations. As a result, it performs (too) well on the training set but poorly on unseen test data because it fails to generalize.\n",
    "</details>\n",
    "\n",
    "**<span style=\"color: green;\">QUESTION:</span>** What is multicollinearity?\n",
    "<details>\n",
    "        <summary>Answer: Click to show</summary>\n",
    "This occurs when two or more independent variables in a regression model are highly correlated with each other. It makes it hard for the model to distinguish their individual effects on the dependent variable, leading to unstable coefficients and unreliable interpretations. In short, predictors overlap too much and confuse the model.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e1eea",
   "metadata": {},
   "source": [
    "## Scaling and scikit-learn Pipelines\n",
    "\n",
    "What is the impact of the scale each feature in the regularisation terms?\n",
    "\n",
    "The scale of each feature affects the scale of it's coefficient. To get the most out of regularisation terms like L1 and L2, we need to adjust the scale of the data using some sort of transformation.\n",
    "The most common transformations for regression are: \n",
    "* **Standard scaling:** $X_{scaled} = \\frac{X - \\mu}{\\sigma}$ where $\\mu$ is the mean and $\\sigma$ is the standard deviation\n",
    "* **MinMax scaling:** $X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}$ which scales features to a [0, 1] range.  \n",
    "\n",
    "It's very important to apply the exact same transformation to the train and the test set.\n",
    "Since the test set is not known to us in advance, we calculate the min, max, mean, std or any other quantity required for scaling using the\n",
    "training set only.  \n",
    "\n",
    "A very effective way to add scaling and other tricks and features in our models and avoid common Data Science pitfalls like target leakage\n",
    "is to use scikit-learn pipelines.\n",
    "\n",
    "Depending on the algorithms and the data, you will need to use different scaling transformations or no scaling at all:\n",
    "* Tree based algorithms don't benefit much from scaling.\n",
    "* Neural networks require the input data to be scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7cdd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn Pipelines with Column Transformer\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Identify numerical columns to scale (only Unit price)\n",
    "numerical_features = [\"Unit price\"]\n",
    "\n",
    "# Create a ColumnTransformer to scale only Unit price\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"num\", StandardScaler(), numerical_features)],  # Scale Unit price\n",
    "    remainder=\"passthrough\",  # Keep categorical and binary columns as they are\n",
    ")\n",
    "\n",
    "# Create Ridge pipeline\n",
    "ridge_pipeline = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"regressor\", Ridge(alpha=1.0))]\n",
    ")\n",
    "\n",
    "# Create Lasso pipeline, using the util function\n",
    "lasso_pipeline = make_pipeline(preprocessor, Lasso(alpha=1.0, max_iter=10000))\n",
    "\n",
    "# show the pipeline graphic interface\n",
    "display(ridge_pipeline)\n",
    "display(lasso_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4396d",
   "metadata": {},
   "source": [
    "In this exercise, we will play around with the Alpha parameter of Ridge and Lasso regression.\n",
    "\n",
    "By adjusting the Alpha parameter, we can control the strength of the regularization applied to the model. A higher Alpha value increases the amount of regularization, which can help prevent overfitting but may also lead to underfitting if set too high. Conversely, a lower Alpha value reduces the regularization effect, allowing the model to fit the training data more closely, but potentially at the cost of generalization to new data.\n",
    "\n",
    "In practice, we can use techniques like cross-validation to find the optimal Alpha value that balances bias and variance, leading to better model performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f21bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge and Lasso Regression Demo\n",
    "@widgets.interact(\n",
    "    alpha=widgets.FloatSlider(\n",
    "        min=0.01, max=10, step=0.01, value=1, description=\"Alpha:\"\n",
    "    ),\n",
    "    method=widgets.RadioButtons(\n",
    "        options=[\"Ridge\", \"Lasso\"], value=\"Ridge\", description=\"Method:\"\n",
    "    ),\n",
    ")\n",
    "def regularization_demo(alpha, method):\n",
    "    if method == \"Ridge\":\n",
    "        reg = Pipeline(\n",
    "            steps=[(\"preprocessor\", preprocessor), (\"regressor\", Ridge(alpha=alpha))]\n",
    "        )\n",
    "    else:\n",
    "        reg = Pipeline(\n",
    "            steps=[\n",
    "                (\"preprocessor\", preprocessor),\n",
    "                (\"regressor\", Lasso(alpha=alpha, max_iter=10000)),\n",
    "            ]\n",
    "        )\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_pred_reg = reg.predict(X_test)\n",
    "    mse_reg = mean_squared_error(y_test, y_pred_reg)\n",
    "    r2_reg = r2_score(y_test, y_pred_reg)\n",
    "    print(f\"{method} Regression - MSE: {mse_reg:.2f}, R^2: {r2_reg:.2f}\")\n",
    "    # Show coefficients\n",
    "    coef = pd.Series(reg.named_steps[\"regressor\"].coef_, index=X_train.columns).round(5)\n",
    "    display(coef.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46f3b9",
   "metadata": {},
   "source": [
    "**<span style=\"color: green;\">QUESTION:</span>** What do you see by adjusting the Alpha parameter?\n",
    "<details>\n",
    "        <summary>Answer: Click to show</summary>\n",
    "\n",
    "By increasing the regularisation strength, we can see that the coefficients of the features are being shrunk towards zero. This means that the model is becoming simpler and less sensitive to the training data, which can help reduce overfitting.\n",
    "\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
