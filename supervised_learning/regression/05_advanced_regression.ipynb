{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f704ea",
   "metadata": {},
   "source": [
    "# Advanced Regression Algorithms\n",
    "\n",
    "This notebook explores more advanced regression algorithms beyond Linear Regression, Ridge, and Lasso. We will use the supermarket sales dataset and compare the performance of several sophisticated models.\n",
    "\n",
    "**Objectives:**\n",
    "- Introduce advanced regression algorithms\n",
    "- Compare their performance using standard metrics\n",
    "- Discuss practical considerations for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e215433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70970f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = '../../data/SuperMarketAnalysis.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a424614",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We will use the same features as before and apply one-hot encoding to categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6f050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "features = ['Unit price', 'Branch', 'Product line', 'Gender', 'Customer type']\n",
    "df_encoded = pd.get_dummies(df[features], drop_first=True)\n",
    "X = df_encoded\n",
    "y = df['Sales']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e597ad7d",
   "metadata": {},
   "source": [
    "## Why Use Advanced regression models?\n",
    "\n",
    "### Our Scenario: Supermarket sales prediction\n",
    "\n",
    "In our previous notebooks, we discovered that **linear models had high MSE on both training and test sets**, indicating **underfitting**. This suggests our supermarket sales data has complex patterns that linear models cannot capture effectively.\n",
    "\n",
    "### Why linear models fall short:\n",
    "\n",
    "1. **Non-linear Relationships**: sales might depend on complex interactions between features\n",
    "   - Example: High-priced items might sell differently across branches\n",
    "   - Customer type + product line combinations might have unique patterns\n",
    "\n",
    "2. **Feature Interactions**: real-world sales involve feature combinations\n",
    "   - Weekend + certain product lines = higher sales\n",
    "   - Gender + product preferences = complex buying patterns\n",
    "\n",
    "3. **Complex Business Logic**: supermarket sales involve:\n",
    "   - Seasonal patterns, customer behavior, location effects\n",
    "   - Price sensitivity varies by product category\n",
    "   - Branch-specific customer preferences\n",
    "\n",
    "Advanced Models can capture:\n",
    "- **Non-linear patterns** in the data\n",
    "- **Feature interactions** automatically\n",
    "- **Complex decision boundaries**\n",
    "- **Better predictions** for business decision-making\n",
    "\n",
    "### Why these specific models?\n",
    "\n",
    "| Model | Why Use for Supermarket Sales | Key Strengths |\n",
    "|-------|------------------------------|---------------|\n",
    "| **Random Forest** | Robust ensemble method, good baseline for business data | Handles mixed data types, reduces overfitting, provides feature importance |\n",
    "| **Gradient Boosting** | Sequential learning captures complex patterns | Excellent predictive performance, handles non-linearity well |\n",
    "| **XGBoost** | Industry standard for structured data like sales records | Optimized performance, built-in regularization, handles missing values |\n",
    "| **LightGBM** | Fast training on business datasets | Memory efficient, excellent for categorical features, fast inference |\n",
    "\n",
    "### Business impact\n",
    "Better sales predictions help with:\n",
    "- **Inventory management**: stock the right products\n",
    "- **Revenue forecasting**: plan budgets and targets\n",
    "- **Customer insights**: understand buying patterns\n",
    "- **Branch optimization**: tailor strategies per location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2861c97",
   "metadata": {},
   "source": [
    "## Advanced Regression Models\n",
    "\n",
    "Let's explore each model and understand why it's suitable for our supermarket sales prediction:\n",
    "\n",
    "### ðŸŒ³ **[Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)**\n",
    "- **How it works**: Combines multiple decision trees, each trained on different data subsets\n",
    "- **Why for sales data**: Excellent at capturing feature interactions (e.g., branch + product line effects)\n",
    "- **Advantages**: Robust to outliers, provides feature importance, reduces overfitting\n",
    "\n",
    "### ðŸ“ˆ **[Gradient Boosting Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)**\n",
    "- **How it works**: Builds models sequentially, each correcting errors from previous ones\n",
    "- **Why for sales data**: Learns complex non-linear patterns in customer behavior\n",
    "- **Advantages**: High predictive accuracy, handles mixed data types well\n",
    "\n",
    "### âš¡ **[XGBoost (Extreme Gradient Boosting)](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/built-in-models/xgboost)**\n",
    "- **How it works**: Optimized gradient boosting with advanced regularization\n",
    "- **Why for sales data**: Industry standard for structured business data like ours\n",
    "- **Advantages**: Built-in regularization, handles missing values, excellent performance\n",
    "\n",
    "### ðŸš€ **[LightGBM (Light Gradient Boosting Machine)](https://lightgbm.readthedocs.io/en/latest/index.html)**\n",
    "- **How it works**: Gradient boosting optimized for speed and memory efficiency\n",
    "- **Why for sales data**: Excels with categorical features (branch, product line, gender)\n",
    "- **Advantages**: Fast training, memory efficient, great for categorical data\n",
    "\n",
    "All these models can automatically discover complex patterns that linear regression missed in our supermarket sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3120d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit models\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42, verbosity=0),\n",
    "    'LightGBM': LGBMRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "def fit_and_evaluate(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return mse, mae, r2\n",
    "\n",
    "results = {name: fit_and_evaluate(model) for name, model in models.items()}\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results, index=['MSE', 'MAE', 'R2']).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f893af24",
   "metadata": {},
   "source": [
    "## Visual Comparison\n",
    "\n",
    "Let's visualize the predictions of each model against the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7611176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    plt.scatter(y_test, y_pred, label=name, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual Sales')\n",
    "plt.ylabel('Predicted Sales')\n",
    "plt.title('Actual vs Predicted Sales (Advanced Models)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a94bb8",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "- Which model performed best on this dataset?\n",
    "\n",
    "<details>\n",
    "        <summary>Answer: Click to show</summary>\n",
    "    All the models performed well on tihs dataset. If comparing MSE scores, Gradient Boosting's performs relativel worse than others, while MAE and R squared are quite similar. This may indicate that the model with higher MSE has larger outlier errors or occasional large mistakes. \n",
    "\n",
    "    MAE treats al errors equally by averaging the absolute values of the residuals. MSE squares the errors, so it penalizes large errors much more than small ones. R squared measures how well the model explains the bariance in the data and is not as sensitive to outliers as the MSE \n",
    "</details>\n",
    "\n",
    "- What could be a next step in understanding further model performances?\n",
    "\n",
    "<details>\n",
    "        <summary>Answer: Click to show</summary>\n",
    "         Hyperparameter tuning and error analysis are possible next steps after trianing the model to further optimize it and to understand why it makes mistakes and how to improve it.\n",
    "</details>\n",
    "\n",
    "- What are the trade-offs between model complexity, interpretability, and performance?\n",
    "\n",
    "<details>\n",
    "        <summary>Answer: Click to show</summary>\n",
    "         Thereâ€™s a trade-off between model complexity, interpretability, and performance. Simple models are easier to understand and explain but may underperform on complex tasks, while complex models can capture more patterns but are harder to interpret and risk overfitting. The best choice depends on the problemâ€”some situations demand transparency, while others prioritize predictive accuracy\n",
    "</details>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d6c109",
   "metadata": {},
   "source": [
    "# Additional Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62da4fc",
   "metadata": {},
   "source": [
    "## Regularization: Ridge and Lasso Regression\n",
    "\n",
    "Regularization techniques help prevent overfitting by adding a penalty to the regression model. This is especially useful when dealing with many features, our model is overfitting or multicollinearity. \n",
    "\n",
    "- **Ridge Regression:** Adds L2 penalty (squared magnitude of coefficients). L2 penalty shrinks the coefficients of the variables, but never makes them zero (disappear completely from the quation). It keeps all features but reduces their influence, spreading effects more evenly.\n",
    "- **Lasso Regression:** Adds L1 penalty (absolute value of coefficients), can shrink some coefficients to zero (feature selection). L1, on the other hand, pushes some coefficients to zero. It does automatically variable sleection and keeps only the most important independent variables in the final equation. In machine learning we say that L1 does _feature (or variable) selection_\n",
    "\n",
    "What is overfitting?\n",
    "<details>\n",
    "        <summary>Answer: Click to show</summary>\n",
    "    Opposite to underfitting, overfitting happens when a model learns not only the true pattern in the trainnig data, but also random fluctuations. As a result, it performs (too) well on the training set but poorly on unseen test data because it fails to generalize.\n",
    "</details>\n",
    "\n",
    "What is multicollinearity?\n",
    "<details>\n",
    "        <summary>Answer: Click to show</summary>\n",
    "    This occurs when two or more independent variables in a regression model are highly correlated with each other. It makes it hard for the model to distinguish their individual effects on the dependent variable, leading to unstable coefficients and unreliable interpretations. In short, predictors overlap too much and confuse the model.\n",
    "</details>\n",
    "\n",
    "These methods help improve model generalization and are widely used in business analytics and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e1eea",
   "metadata": {},
   "source": [
    "## Scaling and scikit-learn Pipelines\n",
    "\n",
    "What is the impact of the scale each feature in the regularisation terms?\n",
    "\n",
    "The scale of each feature affects the scale of it's coefficient. To get the most out of regularisation terms like L1 and L2, we need to adjust the scale of the data using some sort of transformation.\n",
    "The most common transformations for regression are: \n",
    "* **Standard scaling:** $X_{scaled} = \\frac{X - \\mu}{\\sigma}$ where $\\mu$ is the mean and $\\sigma$ is the standard deviation\n",
    "* **MinMax scaling:** $X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}$ which scales features to a [0, 1] range.  \n",
    "\n",
    "A very effective way to add scaling and other tricks and features in our models and avoid common Data Science pitfalls like target leakage\n",
    "is to use scikit-learn pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7cdd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn Pipelines with Column Transformer\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Identify numerical columns to scale (only Unit price)\n",
    "numerical_features = ['Unit price']\n",
    "\n",
    "# Create a ColumnTransformer to scale only Unit price\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', StandardScaler(), numerical_features)],  # Scale Unit price\n",
    "        remainder=\"passthrough\"  # Keep categorical and binary columns as they are\n",
    ")\n",
    "\n",
    "# Create Ridge pipeline\n",
    "ridge_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', Ridge(alpha=1.0))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create Lasso pipeline, using the util function\n",
    "lasso_pipeline = make_pipeline(preprocessor, Lasso(alpha=1.0, max_iter=10000))\n",
    "\n",
    "# show the pipeline graphic interface\n",
    "display(ridge_pipeline)\n",
    "display(lasso_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f21bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge and Lasso Regression Demo\n",
    "@widgets.interact(\n",
    "    alpha=widgets.FloatSlider(min=0.01, max=10, step=0.01, value=1, description='Alpha:'),\n",
    "    method=widgets.RadioButtons(options=['Ridge', 'Lasso'], value='Ridge', description='Method:')\n",
    ")\n",
    "def regularization_demo(alpha, method):\n",
    "    if method == 'Ridge':\n",
    "        reg = Pipeline(\n",
    "            steps=[('preprocessor', preprocessor), ('regressor', Ridge(alpha=alpha))]\n",
    "        )\n",
    "    else:\n",
    "        reg = Pipeline(\n",
    "            steps=[('preprocessor', preprocessor), ('regressor', Lasso(alpha=alpha, max_iter=10000))]\n",
    "        )\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_pred_reg = reg.predict(X_test)\n",
    "    mse_reg = mean_squared_error(y_test, y_pred_reg)\n",
    "    r2_reg = r2_score(y_test, y_pred_reg)\n",
    "    print(f'{method} Regression - MSE: {mse_reg:.2f}, R^2: {r2_reg:.2f}')\n",
    "    # Show coefficients\n",
    "    coef = pd.Series(reg.named_steps['regressor'].coef_, index=X_train.columns)\n",
    "    display(coef.sort_values(ascending=False).head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
